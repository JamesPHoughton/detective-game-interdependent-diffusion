{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of game measures\n",
    "This notebook takes the anonymized data and computes population-level measures for each game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['shuffle', 'f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import itertools\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "\n",
    "from helpers import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results-anonymized/pilot/block_20200505_pilot.json',\n",
       " '../results-anonymized/pilot/block_20200507_pilot.json',\n",
       " '../results-anonymized/pilot/block_20200624_pilot.json',\n",
       " '../results-anonymized/pilot/block_20200626_pilot.json',\n",
       " '../results-anonymized/pilot/block_20200506_pilot.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"../results-anonymized/pilot/\"\n",
    "files = glob.glob(output_dir+'block_*_pilot.json')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = []\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        blocks.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate clues to be used in polarization analysis\n",
    "# final clues used in analysis are connections between hub nodes (1,2) and rim nodes (3-13)\n",
    "t_spokes = ['tclue_1_3', 'tclue_1_4', 'tclue_1_5', 'tclue_1_6', 'tclue_1_7', \n",
    "            'tclue_1_8', 'tclue_1_9', 'tclue_1_10','tclue_1_11', 'tclue_1_12', 'tclue_1_13',\n",
    "            'tclue_2_3', 'tclue_2_4', 'tclue_2_5', 'tclue_2_6', 'tclue_2_7',\n",
    "            'tclue_2_8', 'tclue_2_9', 'tclue_2_10', 'tclue_2_11', 'tclue_2_12', 'tclue_2_13']\n",
    "\n",
    "c_spokes = ['cclue_1_3', 'cclue_1_4', 'cclue_1_5', 'cclue_1_6', 'cclue_1_7', \n",
    "            'cclue_1_8', 'cclue_1_9', 'cclue_1_10','cclue_1_11', 'cclue_1_12', 'cclue_1_13',\n",
    "            'cclue_2_3', 'cclue_2_4', 'cclue_2_5', 'cclue_2_6', 'cclue_2_7',\n",
    "            'cclue_2_8', 'cclue_2_9', 'cclue_2_10', 'cclue_2_11', 'cclue_2_12', 'cclue_2_13']\n",
    "\n",
    "# Enumerate end-of-game survey questions to be used in polarization analysis\n",
    "assessments = ['appearance_1', 'appearance_2', \n",
    "               'clothing_1', 'clothing_2',\n",
    "               'suspect_1', 'suspect_2', 'suspect_3',\n",
    "               'tool_1', 'tool_2', \n",
    "               'vehicle_1', 'vehicle_2']    \n",
    "    \n",
    "def compute_single_point_measures(game):\n",
    "    \"\"\" \n",
    "    Compute the game-level measures \n",
    "    \n",
    "    \"Games\" in this experiment contain both a treatment and control condition\n",
    "    and these must be properly separated from one another.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Form end-of-game survey responses into a dataframe\n",
    "    collector = {}\n",
    "    for p, k in game['players'].items():\n",
    "        try:\n",
    "            collector[k['data.position']] = k['data.caseMade']\n",
    "        except:\n",
    "            print('%s did not complete the post-game survey' %k['data.position'])\n",
    "    responses = pd.DataFrame(collector).T.sort_index()\n",
    "\n",
    "    # Form final notebook states into a dataframe\n",
    "    final_adoptions = pd.DataFrame(data=0, index=responses.index, columns=t_spokes+c_spokes)\n",
    "    for p, k in game['players'].items():\n",
    "        for clue_id in k['data.notebooks']['promising_leads']['clueIDs']:\n",
    "            final_adoptions.loc[k['data.position'], clue_id] = 1\n",
    "\n",
    "\n",
    "    # Determine the number of datapoints to be used in polarization analysis\n",
    "    # if there are missing responses, need to compare equal sized datasets\n",
    "    t_responses = [pos for pos in responses.index if pos.startswith('t')]\n",
    "    c_responses = [pos for pos in responses.index if pos.startswith('c')] \n",
    "    # use whichever condition has fewer responses to set the sample size\n",
    "    n_used = min(len(t_responses), len(c_responses))\n",
    "\n",
    "\n",
    "    def process_subset(subset, spokes):\n",
    "        \"\"\" compute a result on the selected subset of the data \"\"\"\n",
    "        sub_res = {}\n",
    "        \n",
    "        # select the subset of the survey responses that will be used in the subset analysis\n",
    "        sub_survey = responses.loc[subset, assessments]\n",
    "        \n",
    "        # survey PC1 \n",
    "        pca = PCA(n_components=1)\n",
    "        pca.fit(sub_survey)  \n",
    "        sub_res['survey PC1'] = pca.explained_variance_ratio_\n",
    "\n",
    "        # survey similarity percentiles\n",
    "        survey_corrs = sub_survey.T.corr().mask(np.tri(n_used, n_used, 0, dtype='bool')).stack()\n",
    "        sub_res['survey 5% similarity'], sub_res['survey 95% similarity'] = np.percentile(\n",
    "            survey_corrs, [5, 95])\n",
    "        \n",
    "        # select the subset of the behavioral responses that will be used in the subset analysis\n",
    "        sub_adopt = final_adoptions.loc[subset, spokes]\n",
    "        \n",
    "        # final-state PC1\n",
    "        pca = PCA(n_components=1)\n",
    "        pca.fit(sub_adopt)  \n",
    "        sub_res['spoke PC1'] = pca.explained_variance_ratio_\n",
    "        \n",
    "        # final state similarity percentiles\n",
    "        spoke_corrs = sub_adopt.T.corr().mask(np.tri(n_used, n_used, 0, dtype='bool')).stack()\n",
    "        sub_res['spoke 5% similarity'], sub_res['spoke 95% similarity'] = np.percentile(\n",
    "            spoke_corrs, [5, 95])\n",
    "        \n",
    "        # compute the expected values for the given level of adoption\n",
    "        # by shuffling the clues between individuals \n",
    "        # (preserving the number of clues each individual holds, \n",
    "        # and the number of individuals holding each clue)\n",
    "        # do this a number of times and average the result\n",
    "        e95 = []\n",
    "        e5 = []\n",
    "        ePC1 = []\n",
    "        for _ in range(100):\n",
    "            shuffle_adopt = pd.DataFrame(index=sub_adopt.index,\n",
    "                                         columns=sub_adopt.columns,\n",
    "                                         data=shuffle(sub_adopt.values, n=500))\n",
    "\n",
    "            n_agents = len(shuffle_adopt.index)\n",
    "            corrs = shuffle_adopt.astype(float).T.corr().mask(np.tri(n_agents, n_agents, 0, dtype='bool')).stack()\n",
    "            e95.append(np.percentile(corrs, 95))\n",
    "            e5.append(np.percentile(corrs, 5))\n",
    "\n",
    "            pca = PCA(n_components=1)\n",
    "            pca.fit(shuffle_adopt)\n",
    "            ePC1.append(pca.explained_variance_ratio_[0])\n",
    "        \n",
    "        # compute the net effect of (interdependent or independent) diffusion \n",
    "        # over chance distribution of the same clues\n",
    "        sub_res['net spoke PC1'] = sub_res['spoke PC1'] - np.mean(ePC1)\n",
    "        sub_res['net spoke 95% similarity'] = sub_res['spoke 95% similarity'] - np.mean(e95)\n",
    "        sub_res['net spoke 5% similarity'] = sub_res['spoke 5% similarity'] - np.mean(e5)\n",
    "        \n",
    "        return sub_res\n",
    "        \n",
    "        \n",
    "    # For each subset of size 'n_used', compute a result. \n",
    "    # In most cases there are no missing responses, so just compute on the complete set\n",
    "    t_collector = []\n",
    "    for subset in itertools.combinations(t_responses, r=n_used):\n",
    "        t_collector.append(process_subset(subset, t_spokes))\n",
    "\n",
    "    # The recorded result is the average over all subsets\n",
    "    if len(t_collector) > 1:\n",
    "        print('Averaging over %i combinations for treatment case'%len(t_collector))\n",
    "    t_result = pd.DataFrame(t_collector).mean()\n",
    "\n",
    "    # Compute average for confidence and consensus measures on all submissions\n",
    "    t_result['confidence'] = responses.loc[t_responses, 'confidence'].mean()\n",
    "    t_result['consensus'] = responses.loc[t_responses, 'consensus'].mean()    \n",
    "\n",
    "    \n",
    "    # Perform the same analysis as above for the control condition\n",
    "    c_collector = []\n",
    "    for subset in itertools.combinations(c_responses, r=n_used):\n",
    "        c_collector.append(process_subset(subset, c_spokes))\n",
    "\n",
    "    if len(c_collector) > 1:\n",
    "        print('Averaging over %i combinations for control case'%len(c_collector))\n",
    "    c_result = pd.DataFrame(c_collector).mean()\n",
    "\n",
    "    c_result['confidence'] = responses.loc[c_responses, 'confidence'].mean()\n",
    "    c_result['consensus'] = responses.loc[c_responses, 'consensus'].mean()     \n",
    "\n",
    "    \n",
    "    #pd.merge(t_result, c)result, suffixes=(' (inter)', ' (indep)'))\n",
    "    result = pd.concat([t_result, c_result], keys=['inter', 'indep'])\n",
    "    #result['game_id']=game['createdAt'].split('_')[0].replace('-','_').replace(':','_').replace('.','_')\n",
    "    return result\n",
    "\n",
    "def compute_block(block):\n",
    "    results_collector = []\n",
    "    network_collector = []\n",
    "    for name, game in block.items():\n",
    "        network_collector.append('caveman' if 'caveman' in name else 'dodec')\n",
    "        results_collector.append(compute_single_point_measures(game))\n",
    "        \n",
    "    result = pd.concat(results_collector, keys=network_collector)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = pd.concat([compute_block(block) for block in blocks], axis=1)\n",
    "measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_size(measure1, measure2):\n",
    "    return (measure1 - measure2).mean()\n",
    "\n",
    "def p_val(measure1, measure2):\n",
    "    return stats.ttest_rel(measure1, measure2)[1]\n",
    "\n",
    "def make_table(measurements, func):\n",
    "    rows = measurements.index.levels[2]\n",
    "    cols = np.unique(measurements.index.droplevel(2))\n",
    "    res = pd.DataFrame(index=rows, columns=cols)\n",
    "    for row in rows:\n",
    "        for col in cols:\n",
    "            res.at[row, col] = func(measurements.loc[col+tuple([row])], measurements.loc[('dodec', 'indep')+tuple([row])])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(caveman, indep)</th>\n",
       "      <th>(caveman, inter)</th>\n",
       "      <th>(dodec, indep)</th>\n",
       "      <th>(dodec, inter)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>confidence</th>\n",
       "      <td>2.4</td>\n",
       "      <td>6.88333</td>\n",
       "      <td>0</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consensus</th>\n",
       "      <td>3.15</td>\n",
       "      <td>8.71667</td>\n",
       "      <td>0</td>\n",
       "      <td>3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net spoke 5% similarity</th>\n",
       "      <td>-0.0575352</td>\n",
       "      <td>-0.00509524</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0296869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net spoke 95% similarity</th>\n",
       "      <td>0.129209</td>\n",
       "      <td>0.228675</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0193147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net spoke PC1</th>\n",
       "      <td>0.132892</td>\n",
       "      <td>0.178375</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0183607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoke 5% similarity</th>\n",
       "      <td>-0.14851</td>\n",
       "      <td>-0.159686</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0679407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoke 95% similarity</th>\n",
       "      <td>0.106442</td>\n",
       "      <td>0.162549</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0519284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoke PC1</th>\n",
       "      <td>0.0675537</td>\n",
       "      <td>0.0983758</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.030231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survey 5% similarity</th>\n",
       "      <td>-0.198728</td>\n",
       "      <td>-0.142882</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0734243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survey 95% similarity</th>\n",
       "      <td>0.0466303</td>\n",
       "      <td>0.105545</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0270628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survey PC1</th>\n",
       "      <td>0.050675</td>\n",
       "      <td>0.10618</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0242993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         (caveman, indep) (caveman, inter) (dodec, indep)  \\\n",
       "confidence                            2.4          6.88333              0   \n",
       "consensus                            3.15          8.71667              0   \n",
       "net spoke 5% similarity        -0.0575352      -0.00509524              0   \n",
       "net spoke 95% similarity         0.129209         0.228675              0   \n",
       "net spoke PC1                    0.132892         0.178375              0   \n",
       "spoke 5% similarity              -0.14851        -0.159686              0   \n",
       "spoke 95% similarity             0.106442         0.162549              0   \n",
       "spoke PC1                       0.0675537        0.0983758              0   \n",
       "survey 5% similarity            -0.198728        -0.142882              0   \n",
       "survey 95% similarity           0.0466303         0.105545              0   \n",
       "survey PC1                       0.050675          0.10618              0   \n",
       "\n",
       "                         (dodec, inter)  \n",
       "confidence                         4.39  \n",
       "consensus                          3.95  \n",
       "net spoke 5% similarity      -0.0296869  \n",
       "net spoke 95% similarity     -0.0193147  \n",
       "net spoke PC1                 0.0183607  \n",
       "spoke 5% similarity          -0.0679407  \n",
       "spoke 95% similarity         -0.0519284  \n",
       "spoke PC1                     -0.030231  \n",
       "survey 5% similarity         -0.0734243  \n",
       "survey 95% similarity        -0.0270628  \n",
       "survey PC1                    0.0242993  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_table(measurements, effect_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(caveman, indep)</th>\n",
       "      <th>(caveman, inter)</th>\n",
       "      <th>(dodec, indep)</th>\n",
       "      <th>(dodec, inter)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>confidence</th>\n",
       "      <td>0.736415</td>\n",
       "      <td>0.114478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consensus</th>\n",
       "      <td>0.631243</td>\n",
       "      <td>0.0193449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.575628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net spoke 5% similarity</th>\n",
       "      <td>0.0911892</td>\n",
       "      <td>0.823853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.290228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net spoke 95% similarity</th>\n",
       "      <td>0.0280792</td>\n",
       "      <td>0.0636998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.174629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net spoke PC1</th>\n",
       "      <td>0.00891613</td>\n",
       "      <td>0.0493536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.769419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoke 5% similarity</th>\n",
       "      <td>0.263095</td>\n",
       "      <td>0.191935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.870416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoke 95% similarity</th>\n",
       "      <td>0.251753</td>\n",
       "      <td>0.0240253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoke PC1</th>\n",
       "      <td>0.105343</td>\n",
       "      <td>0.138205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.143659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survey 5% similarity</th>\n",
       "      <td>0.0242238</td>\n",
       "      <td>0.237789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.846876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survey 95% similarity</th>\n",
       "      <td>0.536669</td>\n",
       "      <td>0.413217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survey PC1</th>\n",
       "      <td>0.0991641</td>\n",
       "      <td>0.343748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.614008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         (caveman, indep) (caveman, inter) (dodec, indep)  \\\n",
       "confidence                       0.736415         0.114478            NaN   \n",
       "consensus                        0.631243        0.0193449            NaN   \n",
       "net spoke 5% similarity         0.0911892         0.823853            NaN   \n",
       "net spoke 95% similarity        0.0280792        0.0636998            NaN   \n",
       "net spoke PC1                  0.00891613        0.0493536            NaN   \n",
       "spoke 5% similarity              0.263095         0.191935            NaN   \n",
       "spoke 95% similarity             0.251753        0.0240253            NaN   \n",
       "spoke PC1                        0.105343         0.138205            NaN   \n",
       "survey 5% similarity            0.0242238         0.237789            NaN   \n",
       "survey 95% similarity            0.536669         0.413217            NaN   \n",
       "survey PC1                      0.0991641         0.343748            NaN   \n",
       "\n",
       "                         (dodec, inter)  \n",
       "confidence                     0.229662  \n",
       "consensus                      0.575628  \n",
       "net spoke 5% similarity        0.290228  \n",
       "net spoke 95% similarity       0.174629  \n",
       "net spoke PC1                  0.769419  \n",
       "spoke 5% similarity            0.870416  \n",
       "spoke 95% similarity             0.3424  \n",
       "spoke PC1                      0.143659  \n",
       "survey 5% similarity           0.846876  \n",
       "survey 95% similarity           0.69924  \n",
       "survey PC1                     0.614008  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_table(measurements.dropna(axis=1), p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
