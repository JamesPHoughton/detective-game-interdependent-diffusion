{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "# for managing data\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import json\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "from lifelines import CoxTimeVaryingFitter\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "hazard_table_collector = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## July 10 and 18\n",
    "\n",
    "- Game split into two sessions\n",
    "- Floating filler clues\n",
    "- Don't remember if survey asked for a text description or a dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panel_0_treatment_0.00_exp_4_3_4_17_10_220190508_100624 not in experiment\n",
      "panel_0_treatment_0.00_exp_20_3_4_13_10_220190515_104508 not in experiment\n",
      "panel_0_treatment_0.00_exp_20_3_4_13_10_220190515_104508 not in experiment\n",
      "panel_0_treatment_1.00_exp_4_3_4_17_10_220190508_100624 not in experiment\n",
      "panel_0_treatment_1.00_exp_20_3_4_13_10_220190521_101219 not in experiment\n",
      "panel_0_treatment_0.00_exp_20_3_4_13_10_220190708_172010 loaded\n",
      "panel_0_treatment_1.00_exp_20_3_4_13_10_220190708_172010 loaded\n"
     ]
    }
   ],
   "source": [
    "# Load games information\n",
    "export_dir = \"/Users/jameshoughton/Google Drive/MIT PhD/Factionalism_Research/Experiment/empirica/detective-game/design/results/Empirica Data - 2019-07-18 19-27-52/\"\n",
    "players = []\n",
    "with open(export_dir + 'players.json', 'r') as f:\n",
    "    for line in f:\n",
    "        players.append(json.loads(line))\n",
    "\n",
    "games = []\n",
    "with open(export_dir + 'games.json', 'r') as f:\n",
    "    for line in f:\n",
    "        games.append(json.loads(line))\n",
    "        \n",
    "treatments = []\n",
    "with open(export_dir + 'treatments.json', 'r') as f:\n",
    "    for line in f:\n",
    "        treatments.append(json.loads(line))\n",
    "\n",
    "experiment_filepath = \"/Users/jameshoughton/Google Drive/MIT PhD/Factionalism_Research/Experiment/empirica/detective-game/private/exp_20_3_4_13_10_220190708_172010.json\"\n",
    "with open(experiment_filepath, 'r') as f:\n",
    "    experiment = json.load(f)\n",
    "    \n",
    "\n",
    "# match games, players, experiment data\n",
    "experiment_games = []\n",
    "for game in games:\n",
    "    game['players'] = {pl['_id']:pl for pl in players if pl['_id'] in game['playerIds']}\n",
    "    treatment = [t for t in treatments if t[\"_id\"] == game['treatmentId']][0]\n",
    "    game['gameSetupId'] = treatment['name']\n",
    "    \n",
    "    if game['gameSetupId'] in experiment['games'].keys():\n",
    "        game['gameData'] = experiment['games'][game['gameSetupId']]\n",
    "        experiment_games.append(game)\n",
    "        print('%s loaded' %game['gameSetupId'])\n",
    "    else:\n",
    "        print('%s not in experiment' % game['gameSetupId'])\n",
    "        \n",
    "        \n",
    "res = {'game': '0718'}\n",
    "\n",
    "game_0718_control = experiment_games[-1]\n",
    "game = game_0718_control\n",
    "\n",
    "# Define clues used in polarization analysis\n",
    "# final clues used in analysis are connections between hub nodes (1,2) and rim nodes (3-13)\n",
    "c_spokes = ['clue_1_3', 'clue_1_4', 'clue_1_5', 'clue_1_6', 'clue_1_7', \n",
    "            'clue_1_8', 'clue_1_9', 'clue_1_10','clue_1_11', 'clue_1_12', 'clue_1_13',\n",
    "            'clue_2_3', 'clue_2_4', 'clue_2_5', 'clue_2_6', 'clue_2_7',\n",
    "            'clue_2_8', 'clue_2_9', 'clue_2_10', 'clue_2_11', 'clue_2_12', 'clue_2_13']\n",
    "\n",
    "c_pos = game['players'].keys()\n",
    "# Form final notebook states into a dataframe\n",
    "final_adoptions = pd.DataFrame(data=0, index=c_pos, columns=c_spokes)\n",
    "for p, k in game['players'].items():\n",
    "    for clue_id in k['data.notebooks']['promising_leads']['clueIDs']:\n",
    "        final_adoptions.loc[p, clue_id] = 1\n",
    "        \n",
    "# final state PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(final_adoptions.loc[c_pos, c_spokes])  \n",
    "res['c_spoke_pc1_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "# final state similarity percentiles\n",
    "res['c_spoke_5th_similarity'], res['c_spoke_95th_similarity'] = np.percentile(\n",
    "        final_adoptions.loc[c_pos, c_spokes].T.corr().mask(\n",
    "            np.tri(20, 20, 0, dtype='bool')).stack(), [5, 95])\n",
    "\n",
    "\n",
    "game_0718_tmt = experiment_games[-2]\n",
    "game = game_0718_tmt\n",
    "\n",
    "# Define clues used in polarization analysis\n",
    "# final clues used in analysis are connections between hub nodes (1,2) and rim nodes (3-13)\n",
    "t_spokes = ['clue_1_3', 'clue_1_4', 'clue_1_5', 'clue_1_6', 'clue_1_7', \n",
    "            'clue_1_8', 'clue_1_9', 'clue_1_10','clue_1_11', 'clue_1_12', 'clue_1_13',\n",
    "            'clue_2_3', 'clue_2_4', 'clue_2_5', 'clue_2_6', 'clue_2_7',\n",
    "            'clue_2_8', 'clue_2_9', 'clue_2_10', 'clue_2_11', 'clue_2_12', 'clue_2_13']\n",
    "\n",
    "t_pos = game['players'].keys()\n",
    "\n",
    "# Form initial notebook states into a dataframe\n",
    "initial_adoptions = pd.DataFrame(data=0, index=t_pos, columns=t_spokes)\n",
    "for p, k in game['players'].items():\n",
    "    for clue_id in k['data.initialState']['promising_leads']['clueIDs']:\n",
    "        initial_adoptions.loc[p, clue_id] = 1\n",
    "\n",
    "# initial-state PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(initial_adoptions.loc[t_pos, t_spokes])  \n",
    "res['t_spoke_pc1_initial_variance'] = pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "# Form final notebook states into a dataframe\n",
    "final_adoptions = pd.DataFrame(data=0, index=t_pos, columns=t_spokes)\n",
    "for p, k in game['players'].items():\n",
    "    for clue_id in k['data.notebooks']['promising_leads']['clueIDs']:\n",
    "        final_adoptions.loc[p, clue_id] = 1\n",
    "        \n",
    "# final state PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(final_adoptions.loc[t_pos, t_spokes])  \n",
    "res['t_spoke_pc1_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "\n",
    "res['t_spoke_5th_similarity'], res['t_spoke_95th_similarity'] = np.percentile(\n",
    "    final_adoptions.loc[t_pos, t_spokes].T.corr().mask(\n",
    "        np.tri(20, 20, 0, dtype='bool')).stack(), [5, 95])\n",
    "\n",
    "#results.append(res)     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July 26\n",
    "\n",
    "- matched pair\n",
    "- floating (non-spur) dummy clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panel_1_matched_pair_exp_design3_matched_20190723_115409 not in experiment\n",
      "panel_0_matched_pair_exp_design3_matched_20190723_115409 not in experiment\n",
      "panel_0_matched_pair_exp_design3_matched_20190723_115409 not in experiment\n",
      "panel_0_matched_pair_exp_design3_matched_20190726_115504 loaded\n"
     ]
    }
   ],
   "source": [
    "export_dir = \"/Users/jameshoughton/Google Drive/MIT PhD/Factionalism_Research/Experiment/empirica/detective-game/design/results/Empirica Data - 2019-07-26 18-24-03/\"\n",
    "players = []\n",
    "with open(export_dir + 'players.json', 'r') as f:\n",
    "    for line in f:\n",
    "        players.append(json.loads(line))\n",
    "\n",
    "games = []\n",
    "with open(export_dir + 'games.json', 'r') as f:\n",
    "    for line in f:\n",
    "        games.append(json.loads(line))\n",
    "        \n",
    "treatments = []\n",
    "with open(export_dir + 'treatments.json', 'r') as f:\n",
    "    for line in f:\n",
    "        treatments.append(json.loads(line))\n",
    "\n",
    "experiment_filepath = \"/Users/jameshoughton/Google Drive/MIT PhD/Factionalism_Research/Experiment/empirica/detective-game/design/results/exp_design3_matched_20190726_115504.json\"\n",
    "with open(experiment_filepath, 'r') as f:\n",
    "    experiment = json.load(f)\n",
    "    \n",
    "\n",
    "# match games, players, experiment data\n",
    "experiment_games = []\n",
    "for game in games:\n",
    "    game['players'] = {pl['_id']:pl for pl in players if pl['_id'] in game['playerIds']}\n",
    "    treatment = [t for t in treatments if t[\"_id\"] == game['treatmentId']][0]\n",
    "    game['gameSetupId'] = treatment['name']\n",
    "    \n",
    "    if game['gameSetupId'] in experiment['games'].keys():\n",
    "        game['gameData'] = experiment['games'][game['gameSetupId']]\n",
    "        experiment_games.append(game)\n",
    "        print('%s loaded' %game['gameSetupId'])\n",
    "    else:\n",
    "        print('%s not in experiment' % game['gameSetupId'])\n",
    "        \n",
    "game_0726 = experiment_games[-1]\n",
    "\n",
    "game = game_0726\n",
    "\n",
    "# Define clues used in polarization analysis\n",
    "# final clues used in analysis are connections between hub nodes (1,2) and rim nodes (3-13)\n",
    "t_spokes = ['tclue_1_3', 'tclue_1_4', 'tclue_1_5', 'tclue_1_6', 'tclue_1_7', \n",
    "            'tclue_1_8', 'tclue_1_9', 'tclue_1_10','tclue_1_11', 'tclue_1_12', 'tclue_1_13',\n",
    "            'tclue_2_3', 'tclue_2_4', 'tclue_2_5', 'tclue_2_6', 'tclue_2_7',\n",
    "            'tclue_2_8', 'tclue_2_9', 'tclue_2_10', 'tclue_2_11', 'tclue_2_12', 'tclue_2_13']\n",
    "\n",
    "c_spokes = ['cclue_1_3', 'cclue_1_4', 'cclue_1_5', 'cclue_1_6', 'cclue_1_7', \n",
    "            'cclue_1_8', 'cclue_1_9', 'cclue_1_10','cclue_1_11', 'cclue_1_12', 'cclue_1_13',\n",
    "            'cclue_2_3', 'cclue_2_4', 'cclue_2_5', 'cclue_2_6', 'cclue_2_7',\n",
    "            'cclue_2_8', 'cclue_2_9', 'cclue_2_10', 'cclue_2_11', 'cclue_2_12', 'cclue_2_13']\n",
    "\n",
    "t_pos = ['t%i'%i for i in range(20)]\n",
    "c_pos = ['c%i'%i for i in range(20)]\n",
    "pos = t_pos + c_pos\n",
    "\n",
    "# Form initial notebook states into a dataframe\n",
    "initial_adoptions = pd.DataFrame(data=0, index=t_pos+c_pos, columns=t_spokes+c_spokes)\n",
    "for p, k in game['players'].items():\n",
    "    for clue_id in k['data.initialState']['promising_leads']['clueIDs']:\n",
    "        initial_adoptions.loc[k['data.position'], clue_id] = 1\n",
    "\n",
    "# initial-state PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(initial_adoptions.loc[t_pos, t_spokes])  \n",
    "res['t_spoke_pc1_initial_variance'] = pca.explained_variance_ratio_\n",
    "\n",
    "# Form final notebook states into a dataframe\n",
    "final_adoptions = pd.DataFrame(data=0, index=t_pos+c_pos, columns=t_spokes+c_spokes)\n",
    "for p, k in game['players'].items():\n",
    "    for clue_id in k['data.notebooks']['promising_leads']['clueIDs']:\n",
    "        final_adoptions.loc[k['data.position'], clue_id] = 1\n",
    "\n",
    "res = {'game': '0726'}\n",
    "# final state PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(final_adoptions.loc[c_pos, c_spokes])  \n",
    "res['c_spoke_pc1_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(final_adoptions.loc[t_pos, t_spokes])  \n",
    "res['t_spoke_pc1_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "# final state similarity percentiles\n",
    "res['c_spoke_5th_similarity'], res['c_spoke_95th_similarity'] = np.percentile(\n",
    "        final_adoptions.loc[c_pos, c_spokes].T.corr().mask(\n",
    "            np.tri(20, 20, 0, dtype='bool')).stack(), [5, 95])\n",
    "\n",
    "\n",
    "res['t_spoke_5th_similarity'], res['t_spoke_95th_similarity'] = np.percentile(\n",
    "    final_adoptions.loc[t_pos, t_spokes].T.corr().mask(\n",
    "        np.tri(20, 20, 0, dtype='bool')).stack(), [5, 95])\n",
    "\n",
    "results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_spokes = ['tclue_1_3', 'tclue_1_4', 'tclue_1_5', 'tclue_1_6', 'tclue_1_7', \n",
    "            'tclue_1_8', 'tclue_1_9', 'tclue_1_10','tclue_1_11', 'tclue_1_12', 'tclue_1_13',\n",
    "            'tclue_2_3', 'tclue_2_4', 'tclue_2_5', 'tclue_2_6', 'tclue_2_7',\n",
    "            'tclue_2_8', 'tclue_2_9', 'tclue_2_10', 'tclue_2_11', 'tclue_2_12', 'tclue_2_13']\n",
    "\n",
    "c_spokes = ['cclue_1_3', 'cclue_1_4', 'cclue_1_5', 'cclue_1_6', 'cclue_1_7', \n",
    "            'cclue_1_8', 'cclue_1_9', 'cclue_1_10','cclue_1_11', 'cclue_1_12', 'cclue_1_13',\n",
    "            'cclue_2_3', 'cclue_2_4', 'cclue_2_5', 'cclue_2_6', 'cclue_2_7',\n",
    "            'cclue_2_8', 'cclue_2_9', 'cclue_2_10', 'cclue_2_11', 'cclue_2_12', 'cclue_2_13']\n",
    "\n",
    "\n",
    "def who_has_what(g):\n",
    "    holds = []\n",
    "    for n in g:\n",
    "        pos = g.node[n]['pos']\n",
    "        for cl in t_spokes+c_spokes:\n",
    "            edge = game['data.clues'][cl]['nodes']\n",
    "            holds.append((pos, cl, g.node[n]['M'].has_edge(*edge)))\n",
    "\n",
    "    holds_df = pd.DataFrame(holds, columns=['pos', 'clueId', 'present']).pivot(index='pos', columns='clueId', values='present')\n",
    "    #holds_df.reset_index(inplace=True, drop=True)\n",
    "    return holds_df\n",
    "\n",
    "import copy\n",
    "collector = []\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "# for (player_id, g, event) in retrace(game_1127):\n",
    "#     adoptions = who_has_what(g)\n",
    "#     pca.fit(adoptions.loc[t_pos, t_spokes])\n",
    "#     res = {'t':event['t']/60,\n",
    "#            'treatment': 1,\n",
    "#            'pc1':pca.explained_variance_ratio_[0],\n",
    "#            'game': 1127\n",
    "#           }  \n",
    "#     collector.append(res)\n",
    "\n",
    "#     pca.fit(adoptions.loc[c_pos, c_spokes])\n",
    "#     res = {'t': event['t']/60,\n",
    "#            'treatment': 0,\n",
    "#            'pc1': pca.explained_variance_ratio_[0],\n",
    "#            'game': 1127\n",
    "#           }  \n",
    "#     collector.append(res)\n",
    "    \n",
    "# for (player_id, g, event) in retrace(game_1001):\n",
    "#     adoptions = who_has_what(g)\n",
    "#     pca.fit(adoptions.loc[t_pos, t_spokes])\n",
    "#     res = {'t':event['t']/60,\n",
    "#            'treatment': 1,\n",
    "#            'pc1':pca.explained_variance_ratio_[0],\n",
    "#            'game': 1001\n",
    "#           }  \n",
    "#     collector.append(res)\n",
    "\n",
    "#     pca.fit(adoptions.loc[c_pos, c_spokes])\n",
    "#     res = {'t': event['t']/60,\n",
    "#            'treatment': 0,\n",
    "#            'pc1': pca.explained_variance_ratio_[0],\n",
    "#            'game': 1001\n",
    "#           }  \n",
    "#     collector.append(res)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 1\n",
    "\n",
    "- Matched pair\n",
    "- Poorly constructed spur clues\n",
    "- Slider survey - sliders have default values, and force you to move away from center in order to submit a result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panel_1_matched_pair_exp_design3_matched_20190723_115409 not in experiment\n",
      "panel_0_matched_pair_exp_design3_matched_20190723_115409 not in experiment\n",
      "panel_0_matched_pair_exp_design3_matched_20190723_115409 not in experiment\n",
      "panel_0_matched_pair_exp_design3_matched_20190726_115504 not in experiment\n",
      "panel_0_matched_pair_exp_design4_matched_20190925_121500 loaded\n",
      "panel_0_matched_pair_exp_design4_matched_20190925_121500 loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "export_dir = \"/Users/jameshoughton/Google Drive/MIT PhD/Factionalism_Research/Experiment/empirica/detective-game/design/results/Empirica Data - 2019-10-01 18-28-44/\"\n",
    "players = []\n",
    "with open(export_dir + 'players.json', 'r') as f:\n",
    "    for line in f:\n",
    "        players.append(json.loads(line))\n",
    "\n",
    "games = []\n",
    "with open(export_dir + 'games.json', 'r') as f:\n",
    "    for line in f:\n",
    "        games.append(json.loads(line))\n",
    "        \n",
    "treatments = []\n",
    "with open(export_dir + 'treatments.json', 'r') as f:\n",
    "    for line in f:\n",
    "        treatments.append(json.loads(line))\n",
    "\n",
    "experiment_filepath = \"/Users/jameshoughton/Google Drive/MIT PhD/Factionalism_Research/Experiment/empirica/detective-game/design/results/exp_design4_matched_20190925_121500.json\"\n",
    "with open(experiment_filepath, 'r') as f:\n",
    "    experiment = json.load(f)\n",
    "    \n",
    "\n",
    "# match games, players, experiment data\n",
    "experiment_games = []\n",
    "for game in games:\n",
    "    game['players'] = {pl['_id']:pl for pl in players if pl['_id'] in game['playerIds']}\n",
    "    treatment = [t for t in treatments if t[\"_id\"] == game['treatmentId']][0]\n",
    "    game['gameSetupId'] = treatment['name']\n",
    "    \n",
    "    if game['gameSetupId'] in experiment['games'].keys():\n",
    "        game['gameData'] = experiment['games'][game['gameSetupId']]\n",
    "        game['data.clues'] = experiment['games'][game['gameSetupId']]['clues']\n",
    "        experiment_games.append(game)\n",
    "        print('%s loaded' %game['gameSetupId'])\n",
    "    else:\n",
    "        print('%s not in experiment' % game['gameSetupId'])\n",
    "        \n",
    "game_1001 = experiment_games[-1]      \n",
    "\n",
    "\n",
    "game = game_1001\n",
    "res = {'game': '1001'}\n",
    "\n",
    "# Define clues used in polarization analysis\n",
    "# final clues used in analysis are connections between hub nodes (1,2) and rim nodes (3-13)\n",
    "t_spokes = ['tclue_1_3', 'tclue_1_4', 'tclue_1_5', 'tclue_1_6', 'tclue_1_7', \n",
    "            'tclue_1_8', 'tclue_1_9', 'tclue_1_10','tclue_1_11', 'tclue_1_12', 'tclue_1_13',\n",
    "            'tclue_2_3', 'tclue_2_4', 'tclue_2_5', 'tclue_2_6', 'tclue_2_7',\n",
    "            'tclue_2_8', 'tclue_2_9', 'tclue_2_10', 'tclue_2_11', 'tclue_2_12', 'tclue_2_13']\n",
    "\n",
    "c_spokes = ['cclue_1_3', 'cclue_1_4', 'cclue_1_5', 'cclue_1_6', 'cclue_1_7', \n",
    "            'cclue_1_8', 'cclue_1_9', 'cclue_1_10','cclue_1_11', 'cclue_1_12', 'cclue_1_13',\n",
    "            'cclue_2_3', 'cclue_2_4', 'cclue_2_5', 'cclue_2_6', 'cclue_2_7',\n",
    "            'cclue_2_8', 'cclue_2_9', 'cclue_2_10', 'cclue_2_11', 'cclue_2_12', 'cclue_2_13']\n",
    "\n",
    "t_pos = ['t%i'%i for i in range(20)]\n",
    "c_pos = ['c%i'%i for i in range(20)]\n",
    "pos = t_pos + c_pos\n",
    "\n",
    "# Form initial notebook states into a dataframe\n",
    "initial_adoptions = pd.DataFrame(data=0, index=t_pos+c_pos, columns=t_spokes+c_spokes)\n",
    "for p, k in game['players'].items():\n",
    "    for clue_id in k['data.initialState']['promising_leads']['clueIDs']:\n",
    "        initial_adoptions.loc[k['data.position'], clue_id] = 1\n",
    "\n",
    "# initial-state PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(initial_adoptions.loc[t_pos, t_spokes])  \n",
    "res['t_spoke_pc1_initial_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "# Form final notebook states into a dataframe\n",
    "final_adoptions = pd.DataFrame(data=0, index=t_pos+c_pos, columns=t_spokes+c_spokes)\n",
    "for p, k in game['players'].items():\n",
    "    for clue_id in k['data.notebooks']['promising_leads']['clueIDs']:\n",
    "        final_adoptions.loc[k['data.position'], clue_id] = 1\n",
    "\n",
    "# final state PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(final_adoptions.loc[c_pos, c_spokes])  \n",
    "res['c_spoke_pc1_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(final_adoptions.loc[t_pos, t_spokes])  \n",
    "res['t_spoke_pc1_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "# final state similarity percentiles\n",
    "res['c_spoke_5th_similarity'], res['c_spoke_95th_similarity'] = np.percentile(\n",
    "        final_adoptions.loc[c_pos, c_spokes].T.corr().mask(\n",
    "            np.tri(20, 20, 0, dtype='bool')).stack(), [5, 95])\n",
    "\n",
    "\n",
    "res['t_spoke_5th_similarity'], res['t_spoke_95th_similarity'] = np.percentile(\n",
    "    final_adoptions.loc[t_pos, t_spokes].T.corr().mask(\n",
    "        np.tri(20, 20, 0, dtype='bool')).stack(), [5, 95])\n",
    "\n",
    "results.append(res)\n",
    "\n",
    "# for (player_id, g, event) in retrace(game_1001):\n",
    "#     adoptions = who_has_what(g)\n",
    "#     pca.fit(adoptions.loc[t_pos, t_spokes])\n",
    "#     res = {'t':event['t']/60,\n",
    "#            'treatment': 1,\n",
    "#            'pc1':pca.explained_variance_ratio_[0],\n",
    "#            'game': 1001\n",
    "#           }  \n",
    "#     collector.append(res)\n",
    "\n",
    "#     pca.fit(adoptions.loc[c_pos, c_spokes])\n",
    "#     res = {'t': event['t']/60,\n",
    "#            'treatment': 0,\n",
    "#            'pc1': pca.explained_variance_ratio_[0],\n",
    "#            'game': 1001\n",
    "#           }  \n",
    "#     collector.append(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# november 27\n",
    "\n",
    "- with Abdullah\n",
    "- Matched pair \n",
    "- properly tested spurs\n",
    "- bad time logging - turns out I'd been relying on client side clocks to capture events, and so there is no guarantee of synchronization between the different players!\n",
    "- slider survey\n",
    "- problem with slider labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2019-11-27T18:20:38.972Z\n"
     ]
    }
   ],
   "source": [
    "# load game\n",
    "\n",
    "export_dir = \"/Users/jameshoughton/Google Drive/MIT PhD/Factionalism_Research/Experiment/empirica/detective-game/design/results/Empirica Data - 2019-11-27 18-38-43/\"\n",
    "players = []\n",
    "with open(export_dir + 'players.json', 'r') as f:\n",
    "    for line in f:\n",
    "        players.append(json.loads(line))\n",
    "\n",
    "games = []\n",
    "with open(export_dir + 'games.json', 'r') as f:\n",
    "    for line in f:\n",
    "        games.append(json.loads(line))\n",
    "        \n",
    "treatments = []\n",
    "with open(export_dir + 'treatments.json', 'r') as f:\n",
    "    for line in f:\n",
    "        treatments.append(json.loads(line))\n",
    "\n",
    "\n",
    "# match games, players, and treatments\n",
    "loaded_games = []\n",
    "for game in games:\n",
    "    game['players'] = {pl['_id']:pl for pl in players if pl['_id'] in game['playerIds']}\n",
    "    treatment = [t for t in treatments if t[\"_id\"] == game['treatmentId']][0]\n",
    "    game['gameSetupId'] = treatment['name']\n",
    "        \n",
    "    \n",
    "    loaded_games.append(game)\n",
    "    \n",
    "for i, game in enumerate(loaded_games):\n",
    "    print(i, game['createdAt'])\n",
    "    \n",
    "# select which games to process\n",
    "select = [0]\n",
    "process_games = [loaded_games[i] for i in select]\n",
    "\n",
    "game_1127 = process_games[-1] \n",
    "\n",
    "\n",
    "game = game_1127\n",
    "# Define clues used in polarization analysis\n",
    "# final clues used in analysis are connections between hub nodes (1,2) and rim nodes (3-13)\n",
    "t_spokes = ['tclue_1_3', 'tclue_1_4', 'tclue_1_5', 'tclue_1_6', 'tclue_1_7', \n",
    "            'tclue_1_8', 'tclue_1_9', 'tclue_1_10','tclue_1_11', 'tclue_1_12', 'tclue_1_13',\n",
    "            'tclue_2_3', 'tclue_2_4', 'tclue_2_5', 'tclue_2_6', 'tclue_2_7',\n",
    "            'tclue_2_8', 'tclue_2_9', 'tclue_2_10', 'tclue_2_11', 'tclue_2_12', 'tclue_2_13']\n",
    "\n",
    "c_spokes = ['cclue_1_3', 'cclue_1_4', 'cclue_1_5', 'cclue_1_6', 'cclue_1_7', \n",
    "            'cclue_1_8', 'cclue_1_9', 'cclue_1_10','cclue_1_11', 'cclue_1_12', 'cclue_1_13',\n",
    "            'cclue_2_3', 'cclue_2_4', 'cclue_2_5', 'cclue_2_6', 'cclue_2_7',\n",
    "            'cclue_2_8', 'cclue_2_9', 'cclue_2_10', 'cclue_2_11', 'cclue_2_12', 'cclue_2_13']\n",
    "\n",
    "t_pos = ['t%i'%i for i in range(20)]\n",
    "c_pos = ['c%i'%i for i in range(20)]\n",
    "pos = t_pos + c_pos\n",
    "\n",
    "# Form final notebook states into a dataframe\n",
    "final_adoptions = pd.DataFrame(data=0, index=t_pos+c_pos, columns=t_spokes+c_spokes)\n",
    "for p, k in game['players'].items():\n",
    "    for clue_id in k['data.notebooks']['promising_leads']['clueIDs']:\n",
    "        final_adoptions.loc[k['data.position'], clue_id] = 1\n",
    "\n",
    "res = {'game': '1127'}\n",
    "\n",
    "# Form initial notebook states into a dataframe\n",
    "initial_adoptions = pd.DataFrame(data=0, index=t_pos+c_pos, columns=t_spokes+c_spokes)\n",
    "for p, k in game['players'].items():\n",
    "    for clue_id in k['data.initialState']['promising_leads']['clueIDs']:\n",
    "        initial_adoptions.loc[k['data.position'], clue_id] = 1\n",
    "\n",
    "# initial-state PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(initial_adoptions.loc[t_pos, t_spokes])  \n",
    "res['t_spoke_pc1_initial_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "# final state PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(final_adoptions.loc[c_pos, c_spokes])  \n",
    "res['c_spoke_pc1_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(final_adoptions.loc[t_pos, t_spokes])  \n",
    "res['t_spoke_pc1_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "# final state similarity percentiles\n",
    "res['c_spoke_5th_similarity'], res['c_spoke_95th_similarity'] = np.percentile(\n",
    "        final_adoptions.loc[c_pos, c_spokes].T.corr().mask(\n",
    "            np.tri(20, 20, 0, dtype='bool')).stack(), [5, 95])\n",
    "\n",
    "\n",
    "res['t_spoke_5th_similarity'], res['t_spoke_95th_similarity'] = np.percentile(\n",
    "    final_adoptions.loc[t_pos, t_spokes].T.corr().mask(\n",
    "        np.tri(20, 20, 0, dtype='bool')).stack(), [5, 95])\n",
    "\n",
    "results.append(res)\n",
    "\n",
    "# def retrace(game):\n",
    "#     \"\"\"\n",
    "#     Uses the game log and starting conditions to recreate the state of the\n",
    "#     game at every change event.\n",
    "    \n",
    "#     Returns a generator yielding (player_id, g, event) at each event in the game,\n",
    "\n",
    "#     *player_id* is the player logging the event,\n",
    "#     *g* is the state of the game following the event,\n",
    "#     *event* is what is logged, timestamp modified to be in seconds since game start\n",
    "\n",
    "#     Does not return an action if the only change is a list reordering.\n",
    "#     \"\"\"\n",
    "#     #game_data = game['gameData']\n",
    "#     clues = game['data.clues']\n",
    "    \n",
    "#     # create trace social network\n",
    "#     edge_list = []\n",
    "#     for player_id, player_data in game['players'].items():\n",
    "#         for alter_id in player_data['data.alterIDs']:\n",
    "#             edge_list.append([player_id, alter_id])\n",
    "#     g = nx.from_edgelist(edge_list)\n",
    "\n",
    "#     # give trace players starting information\n",
    "#     nx.set_node_attributes(\n",
    "#         g,\n",
    "#         name='pos',  # position in the social network\n",
    "#         values={a: game['players'][a]['data.position'] for a in g}\n",
    "#     )\n",
    "\n",
    "#     nx.set_node_attributes(\n",
    "#         g,\n",
    "#         name='M',  # M for mind/memory\n",
    "#         values={a: nx.from_edgelist([\n",
    "#             clues[bf]['nodes'] for bf in\n",
    "#             #game['players'][a]['data.intialState']['promising_leads']['clueIDs']\n",
    "#             game['players'][a]['data.initialState']['promising_leads']['clueIDs']\n",
    "#         ]) for a in g}\n",
    "#     )\n",
    "\n",
    "#     nx.set_node_attributes(\n",
    "#         g,\n",
    "#         name='F',  # F for forgetory\n",
    "#         values={i: nx.Graph() for i in g}\n",
    "#     )\n",
    "\n",
    "#     # yield the initial state of the experiment\n",
    "#     event = {'at': game['createdAt'],\n",
    "#              't': 0}\n",
    "#     yield (None, g, event)\n",
    "\n",
    "#     # collect the log of the entire experiment by combining all player logs\n",
    "#     game_log = pd.DataFrame()\n",
    "#     for player_id, player_data in game['players'].items():\n",
    "#         player_log = pd.DataFrame(player_data['data.log'])\n",
    "#         player_log['player'] = player_id\n",
    "#         game_log = game_log.append(player_log)\n",
    "#     game_log.sort_values('at', inplace=True)\n",
    "\n",
    "#     # trace game\n",
    "#     t_start = datetime.strptime(game['createdAt'], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "#     for _, event in game_log[game_log['event'] == \"drop\"].iterrows():\n",
    "#         player_id = event[\"player\"]\n",
    "#         source = event['data']['source']\n",
    "#         dest = event['data']['dest']\n",
    "#         if 'clue' in event['data']:\n",
    "#             if event['data']['clue'] != None:\n",
    "#                 edge = clues[event['data']['clue']]['nodes']\n",
    "#             else: # catch incomplete record\n",
    "#                 print('Missing clueID for player %s from source %s at time %s' % (player_id, source, event['at']))\n",
    "#         else:\n",
    "#             print('player %s is missing a clue' % player_id)\n",
    "#             continue\n",
    "#         M = g.node[player_id]['M']\n",
    "#         F = g.node[player_id]['F']\n",
    "#         update = False\n",
    "\n",
    "#         if source == \"promising_leads\":\n",
    "#             assert g.node[event['player']]['M'].has_edge(*edge) # check that clue is still in memory\n",
    "#             if dest == \"dead_ends\":\n",
    "#                 M.remove_edge(*edge)\n",
    "#                 F.add_edge(*edge)\n",
    "#                 update = True\n",
    "\n",
    "#         elif source == \"dead_ends\":\n",
    "#             assert g.node[event['player']]['F'].has_edge(*edge) # check that clue is still in forgettory\n",
    "#             if dest == \"promising_leads\":\n",
    "#                 F.remove_edge(*edge)\n",
    "#                 M.add_edge(*edge)\n",
    "#                 update = True\n",
    "\n",
    "#         else:\n",
    "#             assert source in game['playerIds']  # check that source is another player\n",
    "#             if not g.node[source]['M'].has_edge(*edge):  # check that clue is in source\n",
    "#                 # this can fail if the exposer removes the clue while the exposed is dragging it.\n",
    "#                 # turns out not to be a big deal\n",
    "#                 print(\"%s no longer in source %s\" % (str(edge), str(source)))\n",
    "#             if dest == \"promising_leads\":\n",
    "#                 M.add_edge(*edge)\n",
    "#                 if F.has_edge(*edge):\n",
    "#                     F.remove_edge(*edge)\n",
    "#                 update = True\n",
    "#             elif dest == \"dead_ends\":\n",
    "#                 F.add_edge(*edge)\n",
    "#                 if M.has_edge(*edge):\n",
    "#                     M.remove_edge(*edge)\n",
    "#                 update = True\n",
    "#             assert not (F.has_edge(*edge) and  # not in both memory and forgetery\n",
    "#                         M.has_edge(*edge))\n",
    "\n",
    "#         if update:\n",
    "#             t_current = datetime.strptime(event['at'], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "#             event['t'] = (t_current - t_start).total_seconds()\n",
    "#             yield (player_id, g, event)\n",
    "\n",
    "#     # double check the final state at the end of the generator\n",
    "#     for player_id in g:\n",
    "#         leads = game['players'][player_id]['data.notebooks']['promising_leads']['clueIDs']\n",
    "#         should_have = set([tuple(sorted(clues[clue]['nodes'])) for clue in leads if clue != None])\n",
    "#         has = set([tuple(sorted(edge)) for edge in g.node[player_id]['M'].edges()])\n",
    "#         assert should_have == has\n",
    "\n",
    "#         deads = game['players'][player_id]['data.notebooks']['dead_ends']['clueIDs']\n",
    "#         should_have = set([tuple(sorted(clues[clue]['nodes'])) for clue in deads if clue != None])\n",
    "#         has = set([tuple(sorted(edge)) for edge in g.node[player_id]['F'].edges()])\n",
    "#         assert should_have == has\n",
    "       \n",
    "    \n",
    "    \n",
    "# def who_has_what(g):\n",
    "#     holds = []\n",
    "#     for n in g:\n",
    "#         pos = g.node[n]['pos']\n",
    "#         for cl in t_spokes+c_spokes:\n",
    "#             edge = game['data.clues'][cl]['nodes']\n",
    "#             holds.append((pos, cl, g.node[n]['M'].has_edge(*edge)))\n",
    "\n",
    "#     holds_df = pd.DataFrame(holds, columns=['pos', 'clueId', 'present']).pivot(index='pos', columns='clueId', values='present')\n",
    "#     #holds_df.reset_index(inplace=True, drop=True)\n",
    "#     return holds_df    \n",
    "    \n",
    "\n",
    "# for (player_id, g, event) in retrace(game_1127):\n",
    "#     adoptions = who_has_what(g)\n",
    "#     pca.fit(adoptions.loc[t_pos, t_spokes])\n",
    "#     res = {'t':event['t']/60,\n",
    "#            'treatment': 1,\n",
    "#            'pc1':pca.explained_variance_ratio_[0],\n",
    "#            'game': 1127\n",
    "#           }  \n",
    "#     collector.append(res)\n",
    "\n",
    "#     pca.fit(adoptions.loc[c_pos, c_spokes])\n",
    "#     res = {'t': event['t']/60,\n",
    "#            'treatment': 0,\n",
    "#            'pc1': pca.explained_variance_ratio_[0],\n",
    "#            'game': 1127\n",
    "#           }  \n",
    "#     collector.append(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# December 11th\n",
    "\n",
    "- Matched Pair\n",
    "- Good spur clues\n",
    "- Fixed timing of logs\n",
    "- Slider survey\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2019-11-27T18:20:38.972Z\n",
      "1 2019-12-11T17:08:10.631Z\n",
      "2 2019-12-11T18:21:08.701Z\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-10444c8bc332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;31m## process timeseries parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load game\n",
    "\n",
    "export_dir = \"/Users/jameshoughton/Google Drive/MIT PhD/Factionalism_Research/Experiment/empirica/detective-game/design/results/Empirica Data - 2019-12-11 18-38-30/\"\n",
    "players = []\n",
    "with open(export_dir + 'players.json', 'r') as f:\n",
    "    for line in f:\n",
    "        players.append(json.loads(line))\n",
    "\n",
    "games = []\n",
    "with open(export_dir + 'games.json', 'r') as f:\n",
    "    for line in f:\n",
    "        games.append(json.loads(line))\n",
    "        \n",
    "treatments = []\n",
    "with open(export_dir + 'treatments.json', 'r') as f:\n",
    "    for line in f:\n",
    "        treatments.append(json.loads(line))\n",
    "\n",
    "logs = []\n",
    "with open(export_dir + 'player-logs.json', 'r') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        entry['data'] = json.loads(entry['jsonData'])\n",
    "        logs.append(entry)\n",
    "\n",
    "# match games, players, treatments, and log info\n",
    "loaded_games = []\n",
    "for game in games:\n",
    "    game['players'] = {pl['_id']:pl for pl in players if pl['_id'] in game['playerIds']}\n",
    "    treatment = [t for t in treatments if t[\"_id\"] == game['treatmentId']][0]\n",
    "    game['gameSetupId'] = treatment['name']\n",
    "    game['log'] = [l for l in logs if l['gameId'] == game['_id']]\n",
    "    \n",
    "    loaded_games.append(game)\n",
    "    \n",
    "for i, game in enumerate(loaded_games):\n",
    "    print(i, game['createdAt'])\n",
    "    \n",
    "# select which games to process\n",
    "game = loaded_games[-1] \n",
    "\n",
    "# Define clues used in polarization analysis\n",
    "# final clues used in analysis are connections between hub nodes (1,2) and rim nodes (3-13)\n",
    "t_spokes = ['tclue_1_3', 'tclue_1_4', 'tclue_1_5', 'tclue_1_6', 'tclue_1_7', \n",
    "            'tclue_1_8', 'tclue_1_9', 'tclue_1_10','tclue_1_11', 'tclue_1_12', 'tclue_1_13',\n",
    "            'tclue_2_3', 'tclue_2_4', 'tclue_2_5', 'tclue_2_6', 'tclue_2_7',\n",
    "            'tclue_2_8', 'tclue_2_9', 'tclue_2_10', 'tclue_2_11', 'tclue_2_12', 'tclue_2_13']\n",
    "\n",
    "c_spokes = ['cclue_1_3', 'cclue_1_4', 'cclue_1_5', 'cclue_1_6', 'cclue_1_7', \n",
    "            'cclue_1_8', 'cclue_1_9', 'cclue_1_10','cclue_1_11', 'cclue_1_12', 'cclue_1_13',\n",
    "            'cclue_2_3', 'cclue_2_4', 'cclue_2_5', 'cclue_2_6', 'cclue_2_7',\n",
    "            'cclue_2_8', 'cclue_2_9', 'cclue_2_10', 'cclue_2_11', 'cclue_2_12', 'cclue_2_13']\n",
    "\n",
    "t_pos = ['t%i'%i for i in range(20)]\n",
    "c_pos = ['c%i'%i for i in range(20)]\n",
    "pos = t_pos + c_pos\n",
    "\n",
    "# Form final notebook states into a dataframe\n",
    "final_adoptions = pd.DataFrame(data=0, index=t_pos+c_pos, columns=t_spokes+c_spokes)\n",
    "for p, k in game['players'].items():\n",
    "    for clue_id in k['data.notebooks']['promising_leads']['clueIDs']:\n",
    "        final_adoptions.loc[k['data.position'], clue_id] = 1\n",
    "\n",
    "res = {'game': '1211'}\n",
    "\n",
    "# Form initial notebook states into a dataframe\n",
    "initial_adoptions = pd.DataFrame(data=0, index=t_pos+c_pos, columns=t_spokes+c_spokes)\n",
    "for p, k in game['players'].items():\n",
    "    for clue_id in k['data.initialState']['promising_leads']['clueIDs']:\n",
    "        initial_adoptions.loc[k['data.position'], clue_id] = 1\n",
    "\n",
    "# initial-state PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(initial_adoptions.loc[t_pos, t_spokes])  \n",
    "res['t_spoke_pc1_initial_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "# final state PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(final_adoptions.loc[c_pos, c_spokes])  \n",
    "res['c_spoke_pc1_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(final_adoptions.loc[t_pos, t_spokes])  \n",
    "res['t_spoke_pc1_variance'] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "# final state similarity percentiles\n",
    "res['c_spoke_5th_similarity'], res['c_spoke_95th_similarity'] = np.percentile(\n",
    "        final_adoptions.loc[c_pos, c_spokes].T.corr().mask(\n",
    "            np.tri(20, 20, 0, dtype='bool')).stack(), [5, 95])\n",
    "\n",
    "\n",
    "res['t_spoke_5th_similarity'], res['t_spoke_95th_similarity'] = np.percentile(\n",
    "    final_adoptions.loc[t_pos, t_spokes].T.corr().mask(\n",
    "        np.tri(20, 20, 0, dtype='bool')).stack(), [5, 95])\n",
    "\n",
    "\n",
    "# survey PC1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(responses.loc[subset, assessments])  \n",
    "sub_res['c_survey_pc1_variance'] = pca.explained_variance_ratio_\n",
    "\n",
    "# survey similarity percentiles\n",
    "sub_res['c_survey_5th_similarity'], sub_res['c_survey_95th_similarity'] = np.percentile(\n",
    "    responses.loc[subset, assessments].T.corr().mask(\n",
    "        np.tri(n_used, n_used, 0, dtype='bool')).stack(), [5, 95])\n",
    "\n",
    "results.append(res)\n",
    "\n",
    "assert False\n",
    "## process timeseries parts\n",
    "def retrace(game):\n",
    "    \"\"\"\n",
    "    Uses the game log and starting conditions to recreate the state of the\n",
    "    game at every change event.\n",
    "    \n",
    "    Returns a generator yielding (player_id, g, event) at each event in the game,\n",
    "\n",
    "    *player_id* is the player logging the event,\n",
    "    *g* is the state of the game following the event,\n",
    "    *event* is what is logged, timestamp modified to be in seconds since game start\n",
    "\n",
    "    Does not return an action if the only change is a list reordering.\n",
    "    \"\"\"\n",
    "    #game_data = game['gameData']\n",
    "    clues = game['data.clues']\n",
    "    \n",
    "    # create trace social network\n",
    "    edge_list = []\n",
    "    for player_id, player_data in game['players'].items():\n",
    "        for alter_id in player_data['data.alterIDs']:\n",
    "            edge_list.append([player_id, alter_id])\n",
    "    g = nx.from_edgelist(edge_list)\n",
    "\n",
    "    # give trace players starting information\n",
    "    nx.set_node_attributes(\n",
    "        g,\n",
    "        name='pos',  # position in the social network\n",
    "        values={a: game['players'][a]['data.position'] for a in g}\n",
    "    )\n",
    "\n",
    "    nx.set_node_attributes(\n",
    "        g,\n",
    "        name='M',  # M for mind/memory\n",
    "        values={a: nx.from_edgelist([\n",
    "            clues[bf]['nodes'] for bf in\n",
    "            #game['players'][a]['data.intialState']['promising_leads']['clueIDs']\n",
    "            game['players'][a]['data.initialState']['promising_leads']['clueIDs']\n",
    "        ]) for a in g}\n",
    "    )\n",
    "\n",
    "    nx.set_node_attributes(\n",
    "        g,\n",
    "        name='F',  # F for forgetory\n",
    "        values={i: nx.Graph() for i in g}\n",
    "    )\n",
    "\n",
    "    # yield the initial state of the experiment\n",
    "    yield (None, g, 0)\n",
    "\n",
    "    t_start = datetime.strptime(game['createdAt'], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "    game_log = pd.DataFrame(game['log'])\n",
    "    game_log['logTime'] = game_log['createdAt'].apply(lambda s: datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%fZ'))\n",
    "    game_log['t'] = (game_log['logTime'] - t_start).dt.total_seconds()\n",
    "    game_log.sort_values('t', inplace=True)\n",
    "\n",
    "    # trace game\n",
    "    for _, row in game_log[game_log['name'] == \"drop\"].iterrows():\n",
    "        player_id = row[\"playerId\"]\n",
    "        jsonData = json.loads(row['jsonData'])        \n",
    "        source = jsonData['source']\n",
    "        dest = jsonData['dest']\n",
    "\n",
    "        if 'clue' in jsonData:\n",
    "            if jsonData['clue'] != None:\n",
    "                edge = clues[jsonData['clue']]['nodes']\n",
    "            else: # catch incomplete record\n",
    "                print('Missing clueID for player %s from source %s at time %s' % (player_id, source, row['t']))\n",
    "        else:\n",
    "            print('player %s is missing a clue' % player_id)\n",
    "            continue\n",
    "        M = g.node[player_id]['M']\n",
    "        F = g.node[player_id]['F']\n",
    "        update = False\n",
    "\n",
    "        if source == \"promising_leads\":\n",
    "            assert M.has_edge(*edge) # check that clue is still in memory\n",
    "            if dest == \"dead_ends\":\n",
    "                M.remove_edge(*edge)\n",
    "                F.add_edge(*edge)\n",
    "                update = True\n",
    "\n",
    "        elif source == \"dead_ends\":\n",
    "            assert F.has_edge(*edge) # check that clue is still in forgettory\n",
    "            if dest == \"promising_leads\":\n",
    "                F.remove_edge(*edge)\n",
    "                M.add_edge(*edge)\n",
    "                update = True\n",
    "\n",
    "        else:\n",
    "            assert source in game['playerIds']  # check that source is another player\n",
    "            if not g.node[source]['M'].has_edge(*edge):  # check that clue is in source\n",
    "                # this can fail if the exposer removes the clue while the exposed is dragging it.\n",
    "                # turns out not to be a big deal\n",
    "                # really we should only run this check on 'pickup' events, and then check that a 'drop' \n",
    "                # event follows\n",
    "                print(\"%s no longer in source %s\" % (str(edge), str(source)))\n",
    "            if dest == \"promising_leads\":\n",
    "                M.add_edge(*edge)\n",
    "                if F.has_edge(*edge):\n",
    "                    F.remove_edge(*edge)\n",
    "                update = True\n",
    "            elif dest == \"dead_ends\":\n",
    "                F.add_edge(*edge)\n",
    "                if M.has_edge(*edge):\n",
    "                    M.remove_edge(*edge)\n",
    "                update = True\n",
    "            assert not (F.has_edge(*edge) and  # not in both memory and forgetery\n",
    "                        M.has_edge(*edge))\n",
    "\n",
    "        if update:\n",
    "            yield (player_id, g, row['t'])\n",
    "\n",
    "    # double check the final state at the end of the generator\n",
    "    for player_id in g:\n",
    "        leads = game['players'][player_id]['data.notebooks']['promising_leads']['clueIDs']\n",
    "        should_have = set([tuple(sorted(clues[clue]['nodes'])) for clue in leads if clue != None])\n",
    "        has = set([tuple(sorted(edge)) for edge in g.node[player_id]['M'].edges()])\n",
    "        assert should_have == has\n",
    "\n",
    "        deads = game['players'][player_id]['data.notebooks']['dead_ends']['clueIDs']\n",
    "        should_have = set([tuple(sorted(clues[clue]['nodes'])) for clue in deads if clue != None])\n",
    "        has = set([tuple(sorted(edge)) for edge in g.node[player_id]['F'].edges()])\n",
    "        assert should_have == has\n",
    "       \n",
    "\n",
    "def hazard_factors(game, g, t):\n",
    "    \"\"\"\n",
    "    Given the state of the game, what are the factors that lead to adoption?\n",
    "    \n",
    "    Returns a row for the hazard factors of all individuals adopting all beliefs \n",
    "    at the time of the event, so this gets big fast\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "    players = game['players']\n",
    "    hub_elements = {game['data.nodes']['CrimeScene_1'], game['data.nodes']['StolenObject_1']}\n",
    "    clue_ids = list(game['data.clues'])\n",
    "    spoke_clue_ids = t_spokes+c_spokes  # defined in the outside scope\n",
    "    spoke_edges = [game['data.clues'][clue_id]['nodes'] for clue_id in spoke_clue_ids]\n",
    "    \n",
    "    for player_id in players:\n",
    "        position = players[player_id]['data.position']\n",
    "        neighbors = g.neighbors(player_id)\n",
    "        M = copy.deepcopy(g.node[player_id]['M'])  # promising leads (memory)\n",
    "        F = copy.deepcopy(g.node[player_id]['F'])  # dead ends (forgetory)\n",
    "        \n",
    "        neighbor_clues = {nb: [] for nb in neighbors}\n",
    "        exposers = {clue_id: [] for clue_id in clue_ids}\n",
    "        player_leads = []\n",
    "        player_deads = []\n",
    "        for clue_id in clue_ids:\n",
    "            if clue_id[0] != position[0]: # only collect treatment clues for treatment players, vice versa\n",
    "                continue \n",
    "            edge = game['data.clues'][clue_id]['nodes']\n",
    "            if M.has_edge(*edge):\n",
    "                player_leads.append(clue_id)\n",
    "            if F.has_edge(*edge):\n",
    "                player_deads.append(clue_id)\n",
    "                \n",
    "            for nb in g.neighbors(player_id):\n",
    "                if g.node[nb]['M'].has_edge(*edge):\n",
    "                    neighbor_clues[nb].append(clue_id)\n",
    "                    exposers[clue_id].append(nb)\n",
    "            \n",
    "        fresh_candidates = set([cl for cl, exps in exposers.items() if len(exps) > 0]) - set(player_leads) - set(player_deads)    \n",
    "        n_fresh_candidates = len(fresh_candidates)\n",
    "        \n",
    "        n_existing_beliefs = len(player_leads)\n",
    "        \n",
    "        \n",
    "        # Removing the central clue means that we're only going to count logical interactions\n",
    "        # that go via one of the other spoke clues. Under control conditions there should never \n",
    "        # be a logical interaction. I do this after counting the number of existing beliefs.\n",
    "        if M.has_edge(*tuple(hub_elements)):\n",
    "            M = copy.deepcopy(M)\n",
    "            M.remove_edge(*tuple(hub_elements)) # ignore the clue linking the crime scene to stolen object\n",
    "\n",
    "        \n",
    "        for clue_id in spoke_clue_ids:\n",
    "            if clue_id[0] != position[0]: # only collect treatment clues for treatment players, vice versa\n",
    "                continue \n",
    "                \n",
    "            edge = game['data.clues'][clue_id]['nodes']\n",
    "            rim_node = (set(edge) - hub_elements).pop()\n",
    "            \n",
    "            row = {'game_player_clue_id': \"%s_%s_%s\" % (game['createdAt'], position, clue_id),\n",
    "                   'game_player_id': \"%s_%s\" % (game['createdAt'], position),\n",
    "                   'start': t,\n",
    "                   'treatment': position.startswith('t'),\n",
    "                   'have_belief': clue_id in player_leads,\n",
    "                   'in_deads': 1.0*(clue_id in player_deads),  # cast as float for regression\n",
    "                   'n_exposures': len(exposers[clue_id]),\n",
    "                   'n_existing_beliefs':n_existing_beliefs,\n",
    "                   'n_fresh_candidates': n_fresh_candidates,\n",
    "                   'player_clues': player_leads,\n",
    "                  }\n",
    "            \n",
    "            # number of times the rim element is referenced in the existing belief set\n",
    "            if set(edge) == hub_elements:\n",
    "                row['references'] = np.nan\n",
    "            else:\n",
    "                row['references'] = len(M.edges(rim_node))\n",
    "                \n",
    "                \n",
    "            # count of paths player has connecting ends of clue\n",
    "            path_list = nx.all_simple_paths(M, *edge, cutoff=2) if set(edge)<set(M.nodes()) else []\n",
    "            path_counts = pd.Series(pd.Series([len(pth) - 1 for pth in path_list]).value_counts(),\n",
    "                                    index=range(1, 5)).fillna(0)\n",
    "            row['pl2'] = path_counts[2]\n",
    "\n",
    "            row['exposers_clues'] = {nb: cls for nb, cls in neighbor_clues.items() if nb in exposers[clue_id]}\n",
    "            \n",
    "            rows.append(row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def drop_consecutive_duplicates(df, cols, sort_on):\n",
    "    df = df.sort_values('start')\n",
    "    return df.loc[any(df.shift(-1)[cols] != df[cols], axis=1).shift(+1).fillna('True').astype(bool)]\n",
    "\n",
    "def add_stops(df, t_last):\n",
    "    df['stop'] = df['start'].shift(-1)\n",
    "    df.loc[df.index[-1], 'stop'] = t_last\n",
    "    return df\n",
    "\n",
    "def similarity_func(player_clues, alter_clues):\n",
    "    # fraction of alter's total clues that player holds\n",
    "    return len(set(player_clues) & set(alter_clues)) / len(alter_clues)\n",
    "\n",
    "\n",
    "def process_adoption_group(group):\n",
    "    \"\"\"\n",
    "    Takes the hazard table and creates a table that lifelines can use.\n",
    "    1. Condenses multiple rows (by dropping duplicates)\n",
    "    2. Treats start and end times\n",
    "    3. Identifies adoption events\n",
    "    \"\"\"\n",
    "    \n",
    "    return_cols=[\n",
    "        'treatment', 'game_player_clue_id', 'game_player_id', 'start', 'stop', 'adopt_event', 'forget_event',\n",
    "        'n_exposures', 'n_fresh_candidates', 'have_belief',\n",
    "        'pl2', 'references', 'n_existing_beliefs', 'in_deads',\n",
    "        'max_similarity_to_exposer'\n",
    "    ]\n",
    "    \n",
    "    #indexes at which exposures increase\n",
    "    i_exposures = group.index[group['n_exposures'] > group.shift(+1)['n_exposures'].fillna(0)].tolist()\n",
    "    if len(i_exposures) == 0:  # never exposed, throw out\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    t_first_exposure = group[group['n_exposures']>0]['start'].min()\n",
    "    if t_first_exposure < 1:  # exposed too early, throw out\n",
    "         return pd.DataFrame()\n",
    "    group['t_first_exposure'] = t_first_exposure\n",
    "     \n",
    "    # identify the exposer's clues at the time they expose you to the belief\n",
    "    exposer_exposure_beliefs = {} # beliefs exposer held at the time of exposure\n",
    "    for i in i_exposures:\n",
    "        exposers_clues = group.loc[i]['exposers_clues']  # extract from array\n",
    "        for exposer, clues in exposers_clues.items():\n",
    "            if exposer not in exposer_exposure_beliefs: # new exposure\n",
    "                exposer_exposure_beliefs[exposer] = clues\n",
    "    \n",
    "    # use the list of current exposers to figure out the max similarity to any exposer at the time of exposure\n",
    "    max_similarity_to_exposer = []\n",
    "    for i, row in group.iterrows():\n",
    "        if len(row['exposers_clues']) == 0:\n",
    "            max_similarity_to_exposer.append(-1000)  # this should be nan, but the drop duplicates can't handle that\n",
    "        else:\n",
    "            similarities = [similarity_func(row['player_clues'], exposer_exposure_beliefs[exposer])\n",
    "                            for exposer in row['exposers_clues']]\n",
    "            max_similarity_to_exposer.append(np.max(similarities))\n",
    "    group['max_similarity_to_exposer'] = max_similarity_to_exposer\n",
    "    \n",
    "    # 'stops' for current (short period) rows\n",
    "    t_final = datetime.strptime(game['finishedAt'], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    t_start = datetime.strptime(game['createdAt'], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    total_time = (t_final-t_start).total_seconds()\n",
    "    group = add_stops(group, total_time) \n",
    "    \n",
    "    fgroup = group[\n",
    "        (group['stop'] < 450) & # throw out periods after the right censor\n",
    "        (group['start'] > 1) # throw out before left censor\n",
    "    ] \n",
    "    \n",
    "    if len(fgroup) == 0: # kick out empty groups before we bother with the rest\n",
    "        return pd.DataFrame() \n",
    "        \n",
    "    \n",
    "    fgroup = drop_consecutive_duplicates(\n",
    "        df=fgroup, \n",
    "        cols=['n_exposures', 'max_similarity_to_exposer', 'have_belief', \n",
    "              'references', 'pl2', 'n_existing_beliefs', 'in_deads', \n",
    "              'n_fresh_candidates'], \n",
    "        sort_on='start'\n",
    "    )\n",
    "    fgroup = add_stops(fgroup, total_time) # update stops for longer periods\n",
    "    \n",
    "    fgroup['adopt_event'] = fgroup['have_belief'] < fgroup.shift(-1)['have_belief']\n",
    "    fgroup.loc[fgroup.index[-1], 'adopt_event'] = False\n",
    "    \n",
    "    fgroup['forget_event'] = fgroup['have_belief'] > fgroup.shift(-1)['have_belief']\n",
    "    fgroup.loc[fgroup.index[-1], 'forget_event'] = False\n",
    "    \n",
    "    return fgroup[return_cols]\n",
    "\n",
    "\n",
    "hazard_factors_list = []\n",
    "for (player_id, g, t) in retrace(game):\n",
    "    hazard_factors_list += hazard_factors(game, g, t)\n",
    "hazard_table = pd.DataFrame(hazard_factors_list)\n",
    "hazard_table.sort_values(['game_player_clue_id', 'start'], inplace=True)\n",
    "\n",
    "res_list = []\n",
    "for i, group in hazard_table.groupby('game_player_clue_id'):\n",
    "    res_list.append(process_adoption_group(group))\n",
    "hazard_table = pd.concat(res_list)\n",
    "\n",
    "hazard_table['treatment_n_exposures'] = hazard_table['treatment']*hazard_table['n_exposures']\n",
    "hazard_table['treatment_references'] = hazard_table['treatment']*hazard_table['references']\n",
    "hazard_table['treatment_pl2'] = hazard_table['treatment']*hazard_table['pl2']\n",
    "hazard_table['treatment_n_existing_beliefs'] = hazard_table['treatment']*hazard_table['n_existing_beliefs']\n",
    "hazard_table['treatment_n_fresh_candidates'] = hazard_table['treatment']*hazard_table['n_fresh_candidates']\n",
    "hazard_table['treatment_max_similarity_to_exposer'] = hazard_table['treatment']*hazard_table['max_similarity_to_exposer']\n",
    "hazard_table['treatment_in_deads'] = hazard_table['treatment']*hazard_table['in_deads']*1.0\n",
    "\n",
    "# only look at \n",
    "adoption_hazard_table = hazard_table[(hazard_table['have_belief']==0) & (hazard_table['n_exposures'] > 0)]\n",
    "adoption_hazard_table['adopt_event'] *=1\n",
    "adoption_hazard_table['forget_event'] *=1\n",
    "adoption_hazard_table.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hazard_table_collector.append(adoption_hazard_table)\n",
    "\n",
    "\n",
    "#datafilename = \"adoption_hazard_table.csv\"\n",
    "#adoption_hazard_table.to_csv(datafilename)\n",
    "\n",
    "# def who_has_what(g):\n",
    "#     holds = []\n",
    "#     for n in g:\n",
    "#         pos = g.node[n]['pos']\n",
    "#         for cl in t_spokes+c_spokes:\n",
    "#             edge = game['data.clues'][cl]['nodes']\n",
    "#             holds.append((pos, cl, g.node[n]['M'].has_edge(*edge)))\n",
    "\n",
    "#     holds_df = pd.DataFrame(holds, columns=['pos', 'clueId', 'present']).pivot(index='pos', columns='clueId', values='present')\n",
    "#     #holds_df.reset_index(inplace=True, drop=True)\n",
    "#     return holds_df    \n",
    "    \n",
    "# collector = []\n",
    "# for (player_id, g, t) in retrace(game):\n",
    "#     adoptions = who_has_what(g)\n",
    "#     pca.fit(adoptions.loc[t_pos, t_spokes])\n",
    "#     res = {'t':t/60,\n",
    "#            'treatment': 1,\n",
    "#            'pc1':pca.explained_variance_ratio_[0],\n",
    "#            'game': 1211\n",
    "#           }  \n",
    "#     collector.append(res)\n",
    "\n",
    "#     pca.fit(adoptions.loc[c_pos, c_spokes])\n",
    "#     res = {'t': t/60,\n",
    "#            'treatment': 0,\n",
    "#            'pc1': pca.explained_variance_ratio_[0],\n",
    "#            'game': 1211\n",
    "#           }  \n",
    "#     collector.append(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game</th>\n",
       "      <th>t_spoke_pc1_initial_variance</th>\n",
       "      <th>c_spoke_pc1_variance</th>\n",
       "      <th>t_spoke_pc1_variance</th>\n",
       "      <th>c_spoke_5th_similarity</th>\n",
       "      <th>c_spoke_95th_similarity</th>\n",
       "      <th>t_spoke_5th_similarity</th>\n",
       "      <th>t_spoke_95th_similarity</th>\n",
       "      <th>d5th</th>\n",
       "      <th>d95th</th>\n",
       "      <th>dpc1</th>\n",
       "      <th>dblpc1</th>\n",
       "      <th>pctpc1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.184596</td>\n",
       "      <td>0.293609</td>\n",
       "      <td>0.323265</td>\n",
       "      <td>-0.239046</td>\n",
       "      <td>0.625751</td>\n",
       "      <td>-0.244894</td>\n",
       "      <td>0.692255</td>\n",
       "      <td>-0.005849</td>\n",
       "      <td>0.066504</td>\n",
       "      <td>0.029656</td>\n",
       "      <td>0.109013</td>\n",
       "      <td>0.272044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1127</td>\n",
       "      <td>0.143541</td>\n",
       "      <td>0.279501</td>\n",
       "      <td>0.263857</td>\n",
       "      <td>-0.198557</td>\n",
       "      <td>0.684319</td>\n",
       "      <td>-0.274634</td>\n",
       "      <td>0.628958</td>\n",
       "      <td>-0.076077</td>\n",
       "      <td>-0.055362</td>\n",
       "      <td>-0.015644</td>\n",
       "      <td>0.135961</td>\n",
       "      <td>-0.115064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1211</td>\n",
       "      <td>0.139744</td>\n",
       "      <td>0.259976</td>\n",
       "      <td>0.313525</td>\n",
       "      <td>-0.216591</td>\n",
       "      <td>0.735980</td>\n",
       "      <td>-0.195694</td>\n",
       "      <td>0.623932</td>\n",
       "      <td>0.020897</td>\n",
       "      <td>-0.112048</td>\n",
       "      <td>0.053549</td>\n",
       "      <td>0.120232</td>\n",
       "      <td>0.445382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   game  t_spoke_pc1_initial_variance  c_spoke_pc1_variance  \\\n",
       "0  1001                      0.184596              0.293609   \n",
       "1  1127                      0.143541              0.279501   \n",
       "2  1211                      0.139744              0.259976   \n",
       "\n",
       "   t_spoke_pc1_variance  c_spoke_5th_similarity  c_spoke_95th_similarity  \\\n",
       "0              0.323265               -0.239046                 0.625751   \n",
       "1              0.263857               -0.198557                 0.684319   \n",
       "2              0.313525               -0.216591                 0.735980   \n",
       "\n",
       "   t_spoke_5th_similarity  t_spoke_95th_similarity      d5th     d95th  \\\n",
       "0               -0.244894                 0.692255 -0.005849  0.066504   \n",
       "1               -0.274634                 0.628958 -0.076077 -0.055362   \n",
       "2               -0.195694                 0.623932  0.020897 -0.112048   \n",
       "\n",
       "       dpc1    dblpc1    pctpc1  \n",
       "0  0.029656  0.109013  0.272044  \n",
       "1 -0.015644  0.135961 -0.115064  \n",
       "2  0.053549  0.120232  0.445382  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.DataFrame(results)\n",
    "res_df['d5th'] = res_df['t_spoke_5th_similarity'] - res_df['c_spoke_5th_similarity']\n",
    "res_df['d95th'] = res_df['t_spoke_95th_similarity'] - res_df['c_spoke_95th_similarity']\n",
    "res_df['dpc1'] = res_df['t_spoke_pc1_variance'] - res_df['c_spoke_pc1_variance'] # delta btw control and treatment\n",
    "res_df['dblpc1'] = res_df['c_spoke_pc1_variance'] - res_df['t_spoke_pc1_initial_variance'] # growth of control above baseline\n",
    "res_df['pctpc1'] = res_df['dpc1']/res_df['dblpc1']\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "game                            3.337042e+10\n",
       "t_spoke_pc1_initial_variance    1.559604e-01\n",
       "c_spoke_pc1_variance            2.776954e-01\n",
       "t_spoke_pc1_variance            3.002157e-01\n",
       "c_spoke_5th_similarity         -2.180648e-01\n",
       "c_spoke_95th_similarity         6.820169e-01\n",
       "t_spoke_5th_similarity         -2.384076e-01\n",
       "t_spoke_95th_similarity         6.483814e-01\n",
       "d5th                           -2.034282e-02\n",
       "d95th                          -3.363550e-02\n",
       "dpc1                            2.252035e-02\n",
       "dblpc1                          1.217350e-01\n",
       "pctpc1                          2.007873e-01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c_spoke_pc1_variance       0.016889\n",
       "t_spoke_pc1_variance       0.031862\n",
       "c_spoke_5th_similarity     0.020284\n",
       "c_spoke_95th_similarity    0.055150\n",
       "t_spoke_5th_similarity     0.039868\n",
       "t_spoke_95th_similarity    0.038079\n",
       "d5th                       0.050086\n",
       "d95th                      0.091237\n",
       "dpc1                       0.035144\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.082030\n",
       "1    0.281271\n",
       "2    0.096150\n",
       "3   -0.057583\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res_df['t_spoke_pc1_variance'] - res_df['c_spoke_pc1_variance'])/((res_df['t_spoke_pc1_variance'] + res_df['c_spoke_pc1_variance'])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'game': '1127',\n",
       "  't_spoke_pc1_initial_variance': array([0.14354067]),\n",
       "  'c_spoke_pc1_variance': 0.27950137564179695,\n",
       "  't_spoke_pc1_variance': 0.2638571946633557,\n",
       "  'c_spoke_5th_similarity': -0.19855718635177688,\n",
       "  'c_spoke_95th_similarity': 0.684319353018565,\n",
       "  't_spoke_5th_similarity': -0.2746344822753012,\n",
       "  't_spoke_95th_similarity': 0.6289576373413788},\n",
       " {'game': '1211',\n",
       "  't_spoke_pc1_initial_variance': array([0.1845962]),\n",
       "  'c_spoke_pc1_variance': 0.2599759304969385,\n",
       "  't_spoke_pc1_variance': 0.31352497343973956,\n",
       "  'c_spoke_5th_similarity': -0.21659149724763585,\n",
       "  'c_spoke_95th_similarity': 0.7359800721939875,\n",
       "  't_spoke_5th_similarity': -0.19569405342155582,\n",
       "  't_spoke_95th_similarity': 0.623931623931624},\n",
       " {'game': '1001',\n",
       "  'c_spoke_pc1_variance': 0.2936087578985613,\n",
       "  't_spoke_pc1_variance': 0.32326493401139283,\n",
       "  'c_spoke_5th_similarity': -0.23904572186687872,\n",
       "  'c_spoke_95th_similarity': 0.6257513291440165,\n",
       "  't_spoke_5th_similarity': -0.2448943327776391,\n",
       "  't_spoke_95th_similarity': 0.6922550018933995,\n",
       "  't_spoke_pc1_initial_variance': 0.18459620475010066},\n",
       " {'game': '1001',\n",
       "  'c_spoke_pc1_variance': 0.2936087578985613,\n",
       "  't_spoke_pc1_variance': 0.32326493401139283,\n",
       "  'c_spoke_5th_similarity': -0.23904572186687872,\n",
       "  'c_spoke_95th_similarity': 0.6257513291440165,\n",
       "  't_spoke_5th_similarity': -0.2448943327776391,\n",
       "  't_spoke_95th_similarity': 0.6922550018933995}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'game': '1211',\n",
       " 't_spoke_pc1_initial_variance': array([0.13974429]),\n",
       " 'c_spoke_pc1_variance': 0.2599759304969385,\n",
       " 't_spoke_pc1_variance': 0.31352497343973956,\n",
       " 'c_spoke_5th_similarity': -0.21659149724763585,\n",
       " 'c_spoke_95th_similarity': 0.7359800721939875,\n",
       " 't_spoke_5th_similarity': -0.19569405342155582,\n",
       " 't_spoke_95th_similarity': 0.623931623931624}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
